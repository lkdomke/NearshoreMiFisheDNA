---
title: "wc_spoccupancy"
output: html_document
date: "2025-05-09"
editor_options: 
  chunk_output_type: console
---

libraries
```{r}
#install.packages("spOccupancy")
library(spOccupancy)
library(dplyr)
library(lubridate)
library(rnaturalearth)
library(tidyverse)
library(bayesplot)
library(MCMCvis)
# if rnaturalearth can't be loaded, try:
#install.packages("rnaturalearthhires", repos = "https://ropensci.r-universe.dev", type = "source")
```

# 1) format data for input to spOccupancy

## Read in data
Cleaned in a different script (sourced here)
```{r}
#source("Scripts/spOccupancy_data_cleaning.R")
source(file = "Scripts/mock community analysis/ppc_integratedMsPGOcc.R")
long <- read.csv("Data/spOccupancy_fishdat_long_5-14-25.csv")
sites <- read.csv("Data/spOccupancy_fishdat_sites_5-14-25.csv")

# traits lhs
lhs <- read.csv("Data/NearshoreLHSCharacteristics.csv") %>%
  mutate(Scientific.name = str_trim(Scientific.name, side = "right")) %>%
  mutate(Scientific.name = ifelse(Scientific.name == "Apodicthys flavidus",
                                  "Apodichthys flavidus", Scientific.name)) %>%
  mutate(Pelagic.Demersal = str_trim(Pelagic.Demersal, side = "right")) %>%
  dplyr::rename("name" = Scientific.name,
                "common" = Common.name,
                "schooling" = Schooling.,
                "water_hab" = Pelagic.Demersal,
                "scale" = Scale.Type) %>%
  mutate(schooling = str_replace_all(schooling, " ", "_"),
         scale = str_replace_all(scale, " ", "_")) %>%
  arrange()
```

# 1.1 Dectection-nondetection array

```{r}
long %>%
  distinct(bay_year2, intx, habitat, type, julian, latitude, longitude, year)
# okay this data set looks correct and has all the right data associated
head(long)
edna <- filter(long, type == "edna") %>%
  separate(bay_year, into = c("years", "bay", "sample_id", "replicate")) %>%
  dplyr::select(-c(bay, sample_id, years)) %>%
  filter(value != 0)

seine <- filter(long, type == "seine") %>%
  mutate(replicate = 1) %>%
  dplyr::select(-c(bay_year)) %>%
  filter(value != 0)

# extract codes for each species 
sp.names.edna <- sort(unique(edna$name)) # did not see the same species per gear
sp.names.seine <- sort(unique(seine$name)) # 66 with edna and 49 with seine

# I think maybe we want to include all species in both?
sp.names.all <- sort(unique(c(sp.names.seine,sp.names.edna)))

# extract site codes
site.codes.edna <- sort(unique(edna$bay_year2)) # 37 sites with edna (wo repeating)
site.codes.seine <- sort(unique(seine$bay_year2)) # 35 sites with seine

# length of species
N1 <- length(sp.names.all) # i know these are the same
N2 <- length(sp.names.all)

# maximum number of replicates at a site
K1 <- 3 # up to 3 for edna
K2 <- 1 # only 1 for seine

# number of sites
J1 <- length(unique(edna$bay_year2))
J2 <- length(unique(seine$bay_year2))

# create array for detection non detection data
y.edna <- array(NA, dim = c(N1, J1, K1))
y.seine <- array(NA, dim = c(N2, J2, K2))

# label the dimensions of y 
dimnames(y.edna)[[1]] <- sp.names.all
dimnames(y.edna)[[2]] <- site.codes.edna

dimnames(y.seine)[[1]] <- sp.names.all
dimnames(y.seine)[[2]] <- site.codes.seine



# look at the structure
str(y.edna) # this structure tells us our detection-non detection array will have data for 70 species at 39 sites across a possible 3 replicates at each site. 
str(y.seine) # the main difference here is fewer sites (1:35) and only one replicate (1)

### QUESTION: what is the best way to incorporate the gear type? 
# Should it perhaps be up to 5 replicates at each site?
# Or two different datasets? 
# Based on reading the "integrated multispecies occupancy vignette" lets try with separate datasets
```

For loop to pull in data to fill the NAs in 

For eDNA
```{r}
for (j in 1:J1) { # Loop through sites.
  for (k in 1:K1) { # Loop through replicates at each site.
    # Extract data for current site/replicate combination.
    curr.df <- edna %>%
      filter(bay_year2 == site.codes.edna[j], replicate == k)
 
    # If plot j was sampled during replicate k, 
    # curr.df will have at least 1 row (i.e., at least 
    # one species will be observed). If not, assume it 
    # was not sampled for that replicate.
    if (nrow(curr.df) > 0) {
      # Extract the species that were observed during
      # this site/replicate.
      curr.sp <- which(sp.names.all %in% curr.df$name)
      # Set value to 1 for species that were observed.
      y.edna[curr.sp, j, k] <- 1
      # Set value to 0 for all other species.
      y.edna[-curr.sp, j, k] <- 0
    }
  } # k (replicates)
} # j (sites)

str(y.edna)
apply(y.edna, 1, sum, na.rm = TRUE)
```


For seine
```{r}
for (j in 1:J2) { # Loop through sites.
  for (k in 1:K2) { # Loop through replicates at each site.
    # Extract data for current site/replicate combination.
    curr.df <- seine %>%
      filter(bay_year2 == site.codes.seine[j], replicate == k)
 
    # If plot j was sampled during replicate k, 
    # curr.df will have at least 1 row (i.e., at least 
    # one species will be observed). If not, assume it 
    # was not sampled for that replicate.
    if (nrow(curr.df) > 0) {
      # Extract the species that were observed during
      # this site/replicate.
      curr.sp <- which(sp.names.all %in% curr.df$name)
      # Set value to 1 for species that were observed.
      y.seine[curr.sp, j, k] <- 1
      # Set value to 0 for all other species.
      y.seine[-curr.sp, j, k] <- 0
    }
  } # k (replicates)
} # j (sites)

str(y.seine)
apply(y.seine, 1, sum, na.rm = TRUE)
```

## Check loop set up
```{r}
head(edna)

edna.test <- edna %>%
  mutate(pa = ifelse(value == 0, 0, 1)) %>%
  pivot_wider(names_from = name, values_from = pa, values_fill = 0)

sums <- data.frame(colSums(edna.test[,-c(1:10)])) %>%
  rownames_to_column(var = "names") %>%
  dplyr::rename(sums = "colSums.edna.test....c.1.10...")

test <- apply(y.edna, 1, sum, na.rm = TRUE) %>%
  data.frame() %>%
  rownames_to_column(var = "names") %>%
  dplyr::rename(sums.forloop = ".") %>%
  left_join(sums)

table(test$sums.forloop == test$sums) # good

## test for seines jic
seine.test <- seine %>%
  mutate(pa = ifelse(value == 0, 0, 1)) %>%
  pivot_wider(names_from = name, values_from = pa, values_fill = 0)

sums.seine <- data.frame(colSums(seine.test[,-c(1:10)])) %>%
  rownames_to_column(var = "names") %>%
  dplyr::rename(sums = "colSums.seine.test....c.1.10...")

test2 <- apply(y.seine, 1, sum, na.rm = TRUE) %>%
  data.frame() %>%
  rownames_to_column(var = "names") %>%
  dplyr::rename(sums.forloop = ".") %>%
  left_join(sums.seine)

table(test2$sums.forloop == test2$sums) # good
```


Okay now we've set up our two data arrays for edna (y.edna) and seine (y.seine)
There are different number of sites and species for each of these (see J1, J2 and sp.names)

Next step are setting up the detection covariates
# 1.2 Detection covariates
These covariates first are observation level covariates that account for variation in *detection probability* across different sites and the different surveys at each site. 

We run this model as an intercept only model, so no detection covariates are included. 



# 1.3 Occurence covariates
"formatting covariates is straightforward because covariates *can only vary by site*. Thus they are formatted as a dataframe or matrix. Rows represent the sites and columns represent different covariates

formula:
occr ~ habitat + "region" 
(removed doy)


plot locations & assign a region 
```{r}
library(sf)
head(sites)
table(sites$habitat)

# The polygons I have are nicer and have better detail, but using naturalearth is easier to share
# ak <- st_read(dsn = "../General GIS goodies/alaska_63360/",
#               layer = "ALASKA_63360_PY")

# the polygons from naturalearth also plot way faster than the detailed shapefiles I have
world <- ne_countries(scale = 10, returnclass = "sf")
usa <- ne_states("United States of America", returnclass = "sf") # simple features
ak.small <- subset(usa, name == "Alaska") %>% # Subset ak from the usa object for small AK inset
  st_transform(., crs = st_crs(26908))

sites.sf <- st_as_sf(sites, coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(., crs = st_crs(26908))

# ak.crop <- ak %>%
  # st_transform(., crs = st_crs(26908)) %>%
  # st_crop(ak, xmin = 513765.8, xmax = 760892.3,
  #         ymin = 6054378, ymax = 6247162)

ak.crop <- ak.small %>%
  st_transform(., crs = st_crs(26908)) %>%
  st_crop(ak, xmin = 513765.8, xmax = 760892.3,
          ymin = 6054378, ymax = 6247162)

ggplot() +
  geom_sf(ak.crop, mapping = aes(), fill = "#999999") +
  geom_sf(sites.sf, mapping = aes(fill = habitat),
          size = 5, alpha = 0.8, shape = 21) +
  scale_fill_manual(values = c("#327610", "#E4572E", "#E1AD01"),
                    labels = c("Eelgrass",
                               "Understory kelp",
                               "Mixed eelgrass"),
                    name = "Sample locations")

# whats the best way to dummy assign a region to these? Probably could do it by hand?
# could do ktn area (n = 10), southern tip (n = 6), offshore mid (n = 5), near craig (n = 10), then north the cut (n = 6)

site.region <- sites %>%
  mutate(region = NA,
         region = ifelse(longitude > -132, "Ktn", region),
         region = ifelse(is.na(region) & latitude < 55, "South", region),
         region = ifelse(is.na(region) & latitude > 55.42 & latitude < 55.6, "Craig", region),
         region = ifelse(is.na(region) & latitude > 55.6, "North", region),
         region = ifelse(is.na(region), "Mid", region))

site.region.sf <- st_as_sf(site.region, coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(., crs = st_crs(26908))

ggplot() +
  geom_sf(ak.crop, mapping = aes(), fill = "#999999") +
  geom_sf(site.region.sf, mapping = aes(fill = region),
          size = 5, alpha = 0.8, shape = 21)
```


occr formula: 
occr ~ habitat + region 

rows are sites
```{r}
occ.covs <- site.region %>%
  arrange(bay_year2) %>%
  column_to_rownames(var = "bay_year2") %>%
  dplyr::select(habitat, region)

habitat <- occ.covs[,1]
region <- occ.covs[,2]
str(habitat)
str(region)

str(occ.covs)
```


# 2) Package data into a list
Needs to be in a list format to go into spOccupancy data argument
Should contain: detection-nondection (y), occurence covariates, detection covariates [optional if spatially explicit model: spatial coordinates]

Dection-nondection array
```{r}
str(y.edna) # 37 sites by 70 species over three replicates
str(y.seine) # 35 sites by 70 species over one replicate

# combine to create a full list of two detection arrays
y.full <- list(edna = y.edna,
               seine = y.seine)

str(y.full)
```


Occurence covariates
Either matrix or dataframe
rows length of J (number of sites), column number = # of covariates
```{r}
str(occ.covs) # already a dataframe with two covariates, this is all 37 sites
# which is where all the eDNA sites are, but we need a separate one for the seine
occ.covs.edna <- occ.covs %>%
  rownames_to_column(var = "bay_year2") %>%
  mutate(sites.indx.edna = 1:nrow(.))

occ.covs.seine <- occ.covs %>%
  rownames_to_column(var = "bay_year2") %>%
  mutate(sites.indx.seine = 1:nrow(.)) %>%
  filter(!(bay_year2 %in% c("BTRB_A_2021", "NFEI_B_2021"))) #%>% # this remove index 5, 22
#  column_to_rownames(var = "bay_year2")


str(occ.covs.edna)
str(occ.covs.seine)

# so in the example they combine these using rbind ecause they do not over lap spatially, ours do overlap spatiall and its not clear if we need to use a full occurence covariates dataframe will all sites or seperate out the data source and occurenve covariate
occ.covs.gear <- full_join(occ.covs.edna, occ.covs.seine, by = "bay_year2") %>%
  dplyr::rename(habitat.edna = habitat.x, region.edna = region.x,
                habitat.seine = habitat.y, region.seine = region.y)

str(occ.covs.gear) # df with bayyear and diff columns for each gear covariate
str(occ.covs) # df with bayyear in rownames with all habitat and region covariates

```

Create a site index
sites should be a list with two elements (one for each data source) where each element consists of a vector that indicates the rows in occ.covs that correspond with the specific row of the detection-nondection data for that data source
```{r}
sites.index <- list(edna = occ.covs.edna$sites.indx.edna,
                    seine = occ.covs.seine$sites.indx.seine)
```

Create species index
```{r}
sp.indx <- list(edna = sp.names.all,
                seine = sp.names.all)
```


Package together
```{r}
# package into a list
# data.edna <- list(y = y.edna,
#                   occ.covs = occ.covs
#                   )
# 
# str(data.edna)
# 
# data.seine <- list(y = y.seine,
#                    occ.covs = occ.covs)

data.list <- list(y = y.full,
                  occ.covs = occ.covs, # in example: length is no. of sites in both datasets
                  #no det covs because we're running intcept only model
                  sites = sites.index,
                  species = sp.indx)

str(data.list)
```

# 3) Fit integrated multi-species occupancy model
```{r}
# this takes awhile to run (~ 5 minutes) so don't run it unless you really want to
# out.1 <- intMsPGOcc(occ.formula = ~ as.factor(habitat) + as.factor(region),
#                     det.formula = list(edna = ~ 1,#as.numeric(scale(doy)), # standardizing/centering
#                                        seine = ~1),
#                     data = data.list,
#                     # criteria for MCMC samplers
#                     n.samples = 50000, # number of iterations, default was 6000; 50,000 takes like ~35 min and has nice ESS and Rhats but is a big unweildly for some of the outputs. Can also try 30,000 that is perhaps a better run time. 
#                     # 100,000 had too large of a vector to run
#                     n.omp.threads = 1, # maybe to run chains in parallel change to 3?
#                     verbose = TRUE,
#                     n.report = 2000,
#                     n.burn = 2000, # burn in period?
#                     n.thin = 1, # this is a thinning rate , default was 1
#                     n.chains = 3) # no. of chains

# in generally opting for reading in the object rather re-running the model is probably a good policy
# takes 35 minutes to run and is a large amount of data
# saveRDS(out.1, file = "Data/intMS_output_50000it.rds")
out.1 <- readRDS(file = "Data/intMS_output_50000it.rds") # also takes awhile ~ 5 minutes to save and load this object
summary(out.1)
summary(out.1, level = "community")
summary(out.1, level = "species")
```

Habitat only model
```{r}
# out.2 <- intMsPGOcc(occ.formula = ~ as.factor(habitat),
#                     det.formula = list(edna = ~ 1,#as.numeric(scale(doy)), # standardizing/centering
#                                        seine = ~1), 
#                     data = data.list, 
#                     # criteria for MCMC samplers
#                     n.samples = 10000, # number of iterations, default was 6000
#                     n.omp.threads = 1, # maybe to run chains in parallel change to 3?
#                     verbose = TRUE,
#                     n.report = 200,
#                     n.burn = 2000, # burn in period?
#                     n.thin = 4, # this is a thinning rate , default was 1
#                     n.chains = 3) # no. of chains
# 
# summary(out.2)
```


# 4) Fit checks : intMsPGOcc 
Posterior predictive checks - uses Goodness of Fit (GoF) assessment
If its a good model it should generate data that closely aligns with observed data

If there are drastic differences than the model is likely not very useful
Note: standard approaches for GOF not valid for the raw data in binary response models such as occupancy. So any approach must aggregate the raw values in some manner and then perform model fit assessment on binned values. 

Notes from vignette (https://doserlab.com/files/spoccupancy-web/articles/modelfitting#posterior-predictive-checks):

1. Fit the model using any of the model-fitting functions (here PGOcc()), which generates replicated values for all detection-nondetection data points.
2. Bin both the actual and the replicated detection-nondetection data in a suitable manner, such as by site or replicate (MacKenzie and Bailey 2004).
3. Compute a fit statistic on both the actual data and also on the model-generated ‘replicate data’.
4. Compare the fit statistics for the true data and replicate data. If they are widely different, this suggests a lack of fit of the model to the actual data set at hand.

*Note as in vignette posterior checks haven't been implemented for integrated multispecies models yets*

## Test inclusions of region?
Can calculate WAIC for the model to compare if we specify different formulas
and also by species
```{r}
# waicOcc(out.1) - waicOcc(out.2) # negative meaning that the model with region is preferred. Lets do some checks with that
# small waic indicate models with better performance.
# we're not trying to do out of sample prediction but if we were, we should do a k-fold cross validation approach. 

# waicOcc(out.1, by.sp = T)
```

## check rhat and ESS
Parameters:
beta 
  beta comm - community-level occurrence coefficients
  beta - species level occurrence coefficients
alpha 
  alpha comm - community-level detection coefficients
  alpha - species level detection coefficients
tau
  tau.sq.beta - community-level occurrence variance parameters
  tau.sp.alpha - community-level detection variance parameters
```{r}
# based off of the initial spOccupancy vignette Rhat should be less than 1.1
summary(out.1$rhat$alpha.comm) # below 1.1 good
summary(out.1$rhat$beta.comm) # below 1.1 good
summary(out.1$rhat$tau.sq.beta) # below 1.1 good
summary(out.1$rhat$tau.sq.alpha) # below 1.1 good
summary(out.1$rhat$beta) # gets close but below 1.1
summary(out.1$rhat$alpha) # below 1.1 

# Not exactly sure what ESS should be
# above 1000 - great, 200-1000 medium, <100 low
# medium might indicate some level of autocorrelation
summary(out.1$ESS$alpha.comm) # min 396, max 510, okay; new min 2249 (50,000 iterations)
summary(out.1$ESS$beta.comm) # min 390, max 1150 okay; new min 2145
summary(out.1$ESS$tau.sq.beta) # min 325, max 593 (getting low); new min 1421
summary(out.1$ESS$tau.sq.alpha) # min 375, max 1023 okay; new min 2005
summary(out.1$ESS$beta) # min 197 (don't love this), max 5033 SP level occurrence coef; new min 1331
summary(out.1$ESS$alpha) # min 321, max 6292; new min 2241

```

## MCMC chains convergence: 

```{r}
plot(out.1, param = "beta.comm", density = F)
plot(out.1, param = "alpha.comm", density = F) # intercept only detection
# the alpha comm is... okay but doesn't seem as nice as some of the beta comm

# this rotates through each species but takes a long time with this many species 
# plot(out.1, param = "beta", density = F) # these are spec specific occurence coef
# # this rotates through the species so you have to pay close attention
# plot(out.1, param = "alpha", density = F) # i think decent?, trichodon tichodon isn't the nicest

# i think we can also use the mcmc_trace from 
mcmc_trace(out.1$alpha.comm.samples) #  looks good, detection (intercept only)
mcmc_trace(out.1$beta.comm.samples) # looks good, habitat/region
mcmc_trace(out.1$alpha.samples) # species specific detection kinda hard to tell but looks okay?
# could probably do this neater in a forloop
# mcmc_trace(out.1$beta.samples[,c(1:50)]) # species specific occurrence, showing 50 at a time # questionable ones: Anoplopoma firma, Blepsias
# mcmc_trace(out.1$beta.samples[,c(51:100)]) # looks... okay
# mcmc_trace(out.1$beta.samples[,c(101:170)])
# mcmc_trace(out.1$beta.samples[,c(171:240)])
# mcmc_trace(out.1$beta.samples[,c(241:330)])
# mcmc_trace(out.1$beta.samples[,c(331:400)])
# mcmc_trace(out.1$beta.samples[,c(401:490)])
mcmc_trace(out.1$tau.sq.alpha.samples) # not positive what tau is rn, looks fine
mcmc_trace(out.1$tau.sq.beta.samples) # I don't know about this... bad?; how do we fix this? 
```


## Posterior predictive checks w/ simulated data

From Jeff Doser package developer: If you wanted, you could likely manually derive some different types of GoF tests using the existing functionality by using the detection regression coefficient values (the "alpha.samples") to predict "replicate data" points for the observed data points for each of your different gear types, and then comparing those replicate data predictions to those of the actual data in some way. This is effectively what the ppcOcc function would do. 

Alternatively, you could check out the code from [this](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13811) paper of mine which fit a similar type of model in NIMBLE and included some code for posterior predictive checks.

Shannon took these examples and created a custom function to do this (sourced at beginning of script)
```{r}

ppcOut <- ppcOcc_intMsPGOcc(out.1, "chi-squared", thin.by=100)

# we're looking for values between 0.1 and 0.9 and if they are, we're okay to move forward. 

```

## posterior summaries
```{r}
levels(as.factor(site.region$region)) # ref level craig
levels(as.factor(site.region$habitat)) # ref level eelgrass

MCMCplot(out.1$beta.comm.samples, ref_ovl = T, ci = c(50, 95)) # occurrence parameter estimates compared to the reference level of eelgrass
MCMCplot(out.1$alpha.comm.samples) # intercept only for detection
```


***

# 5) 2-stage model for postHoc testing
Grab the posteriors for detection models (and spit into separate matrices)
(Let's assume you've got a model you like called 'out.1' , all the Rhats and ESS look good, the posterior predictive checks look nice and you're ready to go to your second stage model to see how species traits)

```{r}

# Function to split dataframe at first repeated column name
split_at_first_repeat <- function(df) {
  col_names <- colnames(df)
  
  # Find the first repeated column name
  first_repeat_pos <- which(duplicated(col_names))[1]
  
  if (is.na(first_repeat_pos)) {
    warning("No repeated column names found. Returning original dataframe as first element.")
    return(list(df1 = df, df2 = data.frame()))
  }
  
  # Split the dataframe
  df1 <- df[, 1:(first_repeat_pos - 1), drop = FALSE]
  df2 <- df[, first_repeat_pos:ncol(df), drop = FALSE]
  
  return(list(df1 = df1, df2 = df2))
}

# Usage example:
# Assuming your dataframe is called 'out.1$alpha.samples'
result <- split_at_first_repeat(out.1$alpha.samples)

# Access the two dataframes
detCoefs_edna <- result$df1 
detCoefs_seine <- result$df2  
colnames(detCoefs_edna)
colnames(detCoefs_seine)

nSpecies <- length(sp.names.all)
#doy.Value <- 0 # Set the covariate value; if you've standardized your covs, then set this to zero

detectionEst_eDNA <- 
  detCoefs_edna[,1:nSpecies] #+ # Intercept
  #detCoefs_edna[,(nSpecies+1):(nSpecies*2)] * 0 # 
colnames(detectionEst_eDNA) = sub(".*-","",colnames(detectionEst_eDNA))

#Because seine is intercept only in this example, we dont' need to do anything much to it
detectionEst_seine <- detCoefs_seine 
colnames(detectionEst_seine) = sub(".*-","",colnames(detectionEst_seine))

```

So now we've got our y's set up as needed for the postHocLM model. 
* Note that these estimates are on the logit scale, so our postHoc model will be logit-linear

Next, we need to set up our traits, imagining here that you have two trait "dimensions" that you want to investigate.
``` {r}
# set.seed(8420)

# this is where we bring in our traits
head(lhs)
# make sure the sp match up right
table(sp.names.all == lhs$name) # great

PH.covs <- data.frame(schooling = lhs$schooling,
                      scale = lhs$scale,
                      water_hab = lhs$water_hab) %>%
  mutate(schooling = relevel(as.factor(schooling), ref = "solitary"),
         scale = relevel(as.factor(scale), ref = "scales"),
         water_hab = relevel(as.factor(water_hab), ref = "demersal"))

```


Sew it up into the data format needed for postHocLM() and set up initis and priors
``` {r}

PH.data.eDNA = list(y = detectionEst_eDNA, covs = PH.covs)
PH.data.seine = list(y = detectionEst_seine, covs = PH.covs)

inits <- list(beta = 0, tau.sq = 1)
priors <- list(beta.normal = list(mean = 0, var = 10000),
               tau.sq.ig = c(0.001, 0.001))

```

Now run your model(s)
``` {r}
nSamp.1<-dim(detCoefs_edna)[1]
str(PH.data.eDNA)

post.edna <- postHocLM(formula = ~ scale + schooling + water_hab,
                   data = PH.data.eDNA,
                   inits = inits,
                   priors = priors, 
                   n.samples = nSamp.1, # number of iterations
                   verbose = TRUE,
                   n.report = 200,
                   n.chains = 2)

summary(post.edna)
# bayesian r2 = measure of model fit mean is 23%

post.seine <- postHocLM(formula = ~ scale + schooling + water_hab,
                   data = PH.data.seine,
                   inits = inits,
                   priors = priors, 
                   n.samples = nSamp.1, # number of iterations
                   verbose = TRUE,
                   n.report = 200,
                   n.chains = 2)

summary(post.seine)
# Bayesian r2 = measure of model fit. mean is 20%

```

The coefficients estimated above are on the logit scale. The exponent of a logit-scale coefficient gives you the odds ratio. For example, ignroing the uncertainty for now, if the Mean estimate for behavior "C" is 0.4730...
```{r}
exp(0.4730) #= 1.648
```
 which we interpret as the odds ratio relative to the reference category (behavior "A"). So in this example, after accounting for presence/occupancy, the odds of detecting a "C" fish is 1.6 times as likely as the odds of detecting an "A" fish. 
 
## Check rhat/ESS 
```{r}
# based off of the initial spOccupancy vignette Rhat should be less than 1.1
summary(post.edna$rhat$beta) # below 1.1 good
summary(post.edna$rhat$tau.sq) # 1

summary(post.seine$rhat$beta) # below 1.1 good
summary(post.seine$rhat$tau.sq) # 1

# Not exactly sure what ESS should be
# above 1000 - great, 200-1000 medium, <100 low
# medium might indicate some level of autocorrelation
summary(post.edna$ESS$beta) # min 1332 max 9462, great
summary(post.edna$ESS$tau.sq) # min 455, max 455 okay

summary(post.seine$ESS$beta) # min 1928, max 8761 great
summary(post.seine$ESS$tau.sq) # min/max 833.6
```
 
 
## Model output (Fig 5)
Can we replicate this example to enhance our understanding here
```{r}
library(kableExtra)
levels(as.factor(PH.covs$scale)) # scales are ref
levels(as.factor(PH.covs$schooling)) # solitary ref level
levels(as.factor(PH.covs$water_hab)) # pelagic ref level

# converts it to factor and uses that order
str(post.edna$beta.samples)
str(post.seine$beta.samples)
sum.edna <- summary(post.edna$beta.samples)

edna.summary <- as.data.frame(cbind(sum.edna$statistics[,c(1:2)], exp(sum.edna$statistics[,c(1:2)])))
colnames(edna.summary) <- c("Mean", "SD", "Odds-ratio mean", "Exp SD")

plogis(sum.edna$statistics["(Intercept)",c("Mean", "SD")])

kable(edna.summary, row.names = T, caption = "eDNA detection probabilites") %>%
     column_spec (1:5, border_left = T, border_right = T) %>% 
     kable_styling()

sum.seine <- summary(post.seine$beta.samples)
seine.summary <- as.data.frame(cbind(sum.seine$statistics[,c(1:2)], exp(sum.seine$statistics[,c(1:2)])))
colnames(seine.summary) <- c("Mean", "SD", "Odds-ratio mean", "Exp SD")

plogis(sum.seine$statistics["(Intercept)", c("Mean", "SD")])

kable(seine.summary, row.names = T, caption = "Beach seine detection probabilites") %>%
     column_spec (1:5, border_left = T, border_right = T) %>% 
     kable_styling()


summary(post.seine$beta.samples)

conv.edna <- plogis(post.edna$beta.samples)
conv.seine <- plogis(post.seine$beta.samples)

#png("Images/Trait_Detection_Params.png", height = 8, width = 10, units = "in", res = 400)
MCMCplot(object = post.edna$beta.samples, # enda
         object2 = post.seine$beta.samples, # seine
         ci = c(50, 95), 
         col = "#5E528C", 
         col2 = "#590004", 
         offset = 0.1,
         #guide_lines = T, 
         ref_ovl = T,
         labels = c("Intercept [Scales, Solitary, Demersal]", 
                    "Deciduous scales", "No scales", "Plates", "Some scales", 
                    "Faculative schooling", "Obligatory schooling", "Both demersal/pelagic", "Pelagic"),
         sz_labels = 1.2,
         sz_med = 2, sz_thick = 6, sz_thin = 3)
legend(x = "topright", legend =c("eDNA", "Beach seines"),
       fill = c("#9c91ca", "#ac6162"), yjust = 1,
       title = "Gear type", bty = "n", cex = 1.2)
#dev.off()

# so for this one the sign variable are plates which has a mean of -4.5
#exp(-4.5595)  = 0.01046
# so after accounting for presence/occupancy the odds of detecting a fish with plates compared to the reference level of a fish (facultative, with deciduous scale, both pelagic and benthic) then its 0.01 as likely as detecting the ref level fish? See email from shannon for more information

#MCMCplot(out.22$beta.samples, ref_ovl = T, ci = c(50,95))
MCMCplot(post.edna$beta.samples, ref_ovl = T, ci = c(50,95))
MCMCplot(post.seine$beta.samples, ref_ovl = T, ci = c(50,95))
#MCMCplot(out.32$beta.samples, ref_ovl = T, ci = c(50,95))

```

