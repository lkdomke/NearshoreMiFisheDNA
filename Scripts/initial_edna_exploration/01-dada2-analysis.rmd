---
title: "01-dada2-analysis"
output: html_notebook
---

4 Feb 2022

Running Lia's MiFish data through DADA2

```{r load-libraries}
library(dada2)
library(dplyr)
```

```{r}
# file location
path <- "/genetics/edna/workdir/liaUAF/trimmed"

path
list.files(path)
```


```{r}
fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

```{r}
plotQualityProfile(fnFs[1:2])
plotQualityProfile(fnRs[1:2])
```
Based on these plots, I'd trim FWD reads at 145 bp and REV at 125 bp


```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Initial filtering truncLen failed miserably. We changed it to be super-stringent to see how that affects downstream processing.
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(100,100),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 
head(out)
```
[I moved those files with no reads over and re-ran the above analysis without them]
Figure out which files have no reads and remove them from the file list - otherwise the dada2 algorithm (below) cannot be implemented.

```{r}
# filter the matrix to retain samples with >0 reads
tossers <- out[out[,2] == 0,]
```


```{r}
# tosser.names <- rownames(tossers)
# # move a set of files
# file.copy(from=file.path(paste0(path,"/", tosser.names)),
#           to=file.path(paste0(path, "/tossers","/", tosser.names)))
# # remove those from the original directory
# file.remove(from=file.path(paste0(path,"/",tosser.names)))

```



Let's move fwd with these for now... and come back if there are other issues.

### Error rates
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)

```

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)

```

```{r}
plotErrors(errF, nominalQ=TRUE)

```

```{r}
plotErrors(errR, nominalQ=TRUE)

```

### Sample inference

```{r}
# forwards
dadaFs <- dada(filtFs, err=errF, pool="pseudo", multithread=TRUE)


# reverses
dadaRs <- dada(filtRs, err=errR, pool="pseudo", multithread=TRUE)


```

Merge paired end reads
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

Make a sequence table
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
Let's remove the singletons and off-target sequences
```{r}
seqtab2 <- seqtab[,nchar(colnames(seqtab)) >166]
```

Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
Calculate frequency of chimeras
```{r}
sum(seqtab.nochim)/sum(seqtab2)

```
Chimeras only make up ~4-5% of reads. That seems ok.

Track reads through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

Looks good! We kept the majority of our raw reads, and there is no over-large drop associated with any single step.

## Export files for taxonomy and samples/ASVs

```{r regseqs-asv-output}
 #make fasta file with ASVs
    asv_seqs=colnames(seqtab.nochim)
    for(i in 1:length(asv_seqs))
    {
        write.table(paste(">ASV",i, sep=""),file="csv_outputs/MiFish_ASV_seqtab_nochim.csv", append=TRUE, col.names = F, row.names = F, quote=F)
        write.table(paste(asv_seqs[i], sep=""),file="csv_outputs/MiFish_ASV_seqtab_nochim.csv", append=TRUE, col.names = F, row.names = F, quote=F)
    }
```

That's the input for the FASTA blastn search.


Goal: change ASV headers to numbered ASVs that correspond to those output in the FASTA file.
```{r first-for-poolseqs}
# Make map between brief names and full sequences
briefToSeq <- colnames(seqtab.nochim)
names(briefToSeq) <- paste0("Seq", seq(ncol(seqtab.nochim))) # Seq1, Seq2, ...
# Make new sequence table with brief names
st.brief <- seqtab.nochim
colnames(st.brief) <- names(briefToSeq)

# export the pool seq table with brief names:
write.csv(st.brief, file="csv_outputs/MiFish_ASVtable.csv")
```